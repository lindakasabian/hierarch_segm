{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test task solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some code was borrowed from https://github.com/qubvel-org/segmentation_models.pytorch/blob/main/examples/camvid_segmentation_multiclass.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solve semantic segmentation with hierarchy of the body. given three metrics on pascal part dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mIoU<sup>0</sup> - `body`\n",
    "* mIou<sup>1</sup> - `upper_body`, `lower_body`\n",
    "* mIoU<sup>2</sup> - `low_hand`, `up_hand`, `torso`, `head`, `low_leg`, `up_leg`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most straightforward solutions is to use multiple heads of pretrained model. Another one is to make a cascade of models and combine them into one torch Module. \n",
    "\n",
    "I tried both solutions and the second one is a no to me due to low RAM in my computer (6GBs) and constant heating issues with long training times\n",
    "\n",
    "I will use first one then because it's more convenient for me to only modify segmentation head, thus having better training speed and I can only modify the last output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "from torchvision import tv_tensors\n",
    "import torch\n",
    "\n",
    "class PascalDS(Dataset):\n",
    "    \"\"\"\n",
    "    Pascal part dataset with on the fly mask creation\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list_path, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(file_list_path, \"r\") as f:\n",
    "            self.filenames = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "        Create masks on the fly based on classes. Transform applies to all image and masks\n",
    "        and filters what to use inside torchvision transforms class to prevent scaling integer masks\n",
    "        \"\"\"\n",
    "        \n",
    "        img_filename = self.filenames[idx]\n",
    "        img_basename = img_filename.split(\"/\")[-1]\n",
    "\n",
    "        img_path = os.path.join(\n",
    "            \"/mnt/c/datasets/pascal_part/JPEGImages\", img_filename + \".jpg\"\n",
    "        )\n",
    "        mask_path = os.path.join(\n",
    "            \"/mnt/c/datasets/pascal_part/gt_masks\", img_basename + \".npy\"\n",
    "        )\n",
    "\n",
    "        image = decode_image(img_path)\n",
    "\n",
    "        full_mask = np.load(mask_path)\n",
    "        # consider any part of body as body or class 1\n",
    "        binary_mask = (full_mask > 0).astype(np.uint8)\n",
    "\n",
    "        upper = [4, 2, 1, 6]\n",
    "        lower = [3, 5]\n",
    "        triple_mask = np.zeros_like(full_mask, dtype=np.uint8)\n",
    "        # upper body of triple is 1\n",
    "        triple_mask[np.isin(full_mask, upper)] = 1\n",
    "        # and lower is 2\n",
    "        triple_mask[np.isin(full_mask, lower)] = 2\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, binary_mask, triple_mask, full_mask = self.transform(\n",
    "                image, binary_mask, \n",
    "            triple_mask, full_mask\n",
    "                )\n",
    "            \n",
    "            full_mask = full_mask.long()\n",
    "            binary_mask = binary_mask.long()\n",
    "            triple_mask = triple_mask.long()\n",
    "\n",
    "\n",
    "        return image, binary_mask, triple_mask, full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/mnt/c/datasets/pascal_part\"\n",
    "\n",
    "images_dir = f\"{root_path}/JPEGImages\"\n",
    "masks_dir = f\"{root_path}/gt_masks\"\n",
    "\n",
    "# I don't use strong augmentations here because with multiple heads manual channel\n",
    "# alignment in any kind of interpolation needed in order\n",
    "# to properly use them and it's not my goal here\n",
    "# simple augs are safe\n",
    "\n",
    "train_tfs = T.Compose(\n",
    "    [\n",
    "        T.ToImage(),\n",
    "        T.RandomResizedCrop((256, 256)),\n",
    "        T.RandomAdjustSharpness(p=0.4, sharpness_factor=1),\n",
    "        T.RandomAffine(degrees=25),\n",
    "        T.GaussianBlur(kernel_size=5),\n",
    "        T.RandomHorizontalFlip(p=1),\n",
    "T.ToDtype(dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.uint8, \"others\":\n",
    "    torch.int64}, scale=False),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tfs = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToDtype(dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.uint8, \"others\":\n",
    "    torch.int64}, scale=False),\n",
    "])\n",
    "\n",
    "# Combine into a dataset\n",
    "train_ds = PascalDS(\n",
    "    file_list_path=\"/mnt/c/datasets/pascal_part/train_id.txt\",\n",
    "    transform=train_tfs)\n",
    "\n",
    "test_ds = PascalDS(\n",
    "    file_list_path=\"/mnt/c/datasets/pascal_part/val_id.txt\",\n",
    "    transform=test_tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_mask(\n",
    "    image_path: str, mask_path: str, output_path: str, alpha: float = 0.5\n",
    "):\n",
    "    \"\"\"  \n",
    "    function to visualize file image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    # explicit class mapping\n",
    "    label_names = {\n",
    "        0: \"bg\",\n",
    "        1: \"low_hand\",\n",
    "        2: \"torso\",\n",
    "        3: \"low_leg\",\n",
    "        4: \"head\",\n",
    "        5: \"up_leg\",\n",
    "        6: \"up_hand\",\n",
    "    }\n",
    "\n",
    "    # explicit color selection\n",
    "    label_colors = np.array(\n",
    "        [\n",
    "            [0, 0, 0],  # bg\n",
    "            [255, 0, 0],  # low_hand R\n",
    "            [0, 255, 0],  # torso G\n",
    "            [0, 0, 255],  # low_leg B\n",
    "            [255, 255, 0],  # head Y\n",
    "            [255, 0, 255],  # up_leg P\n",
    "            [0, 255, 255],  # up_hand Cyan\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "    mask = np.load(mask_path)\n",
    "\n",
    "    color_mask = label_colors[mask]\n",
    "\n",
    "    # blending factor to apply transparency\n",
    "    blended = (1 - alpha) * image + alpha * color_mask\n",
    "    blended = blended.astype(np.uint8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.imshow(blended)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    unique_labels = np.unique(mask)\n",
    "    for label_id in unique_labels:\n",
    "        if label_id == 0:\n",
    "            continue\n",
    "\n",
    "        coords = np.argwhere(mask == label_id)\n",
    "        if coords.size == 0:\n",
    "            continue\n",
    "\n",
    "        y_mean, x_mean = coords.mean(axis=0)\n",
    "\n",
    "        label_text = label_names.get(label_id, f\"Class {label_id}\")\n",
    "\n",
    "        ax.text(\n",
    "            x_mean,\n",
    "            y_mean,\n",
    "            label_text,\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"black\", alpha=0.5),\n",
    "        )\n",
    "\n",
    "    fig.savefig(output_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/linda/mil_test/test.png\n"
     ]
    }
   ],
   "source": [
    "img, full_mask = (\n",
    "    \"/mnt/c/datasets/pascal_part/JPEGImages/2008_000034.jpg\",\n",
    "    \"/mnt/c/datasets/pascal_part/gt_masks/2008_000034.npy\",\n",
    ")\n",
    "\n",
    "output_path = \"/home/linda/mil_test/test.png\"\n",
    "\n",
    "print_mask(img, full_mask, output_path, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use segformer from segmentation_models_pytorch (SMP further) as it's good in metrics and it can be modularly replaced with another model. It's code defined outside of this notebook in segformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linda/miniconda3/envs/vis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from segformer import Segformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load and check our model. I will train in lightning so it's just an example of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segformer(\n",
    "    encoder_name=\"mit_b3\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights=\"imagenet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can see multiple heads instead of one. let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segformer(\n",
       "  (encoder): MixVisionTransformerEncoder(\n",
       "    (patch_embed1): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (patch_embed2): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (patch_embed3): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (patch_embed4): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.004)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.007)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    (block2): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.011)\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.015)\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.019)\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.022)\n",
       "        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (block3): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.026)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.030)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.033)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.037)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.041)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.044)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.048)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.052)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.056)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.059)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.063)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.067)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.070)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.074)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.078)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.081)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.085)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.089)\n",
       "        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "    (block4): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.093)\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.096)\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.100)\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): SegformerDecoder(\n",
       "    (mlp_stage): ModuleList(\n",
       "      (0): MLP(\n",
       "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): MLP(\n",
       "        (linear): Linear(in_features=320, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): MLP(\n",
       "        (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): MLP(\n",
       "        (linear): Linear(in_features=64, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fuse_stage): Conv2dReLU(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (seg_binary_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       "  (seg_triple_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       "  (seg_full_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use fully decoupled lr from optimi package and stable adamw from there to ensure no losses go nan during bf16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "bs = 23\n",
    "lr = 7e-5\n",
    "eps = 1e-6\n",
    "# for decoupled wd\n",
    "w_reg = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=12)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "params = smp.encoders.get_preprocessing_params(\"mit_b3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get encoder mean and std to normalize our image feeded into network according to pretraining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.485, 0.456, 0.406]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning code (actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-optimi in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch>=1.13 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch-optimi) (2.5.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch-optimi) (24.2)\n",
      "Requirement already satisfied: filelock in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from torch>=1.13->torch-optimi) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13->torch-optimi) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/linda/miniconda3/envs/vis/lib/python3.10/site-packages (from jinja2->torch>=1.13->torch-optimi) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-optimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimi import StableAdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to interesting parts - we need multiple losses for our multihead beast as they are solving different tasks and gradient update should be done on them separately. \n",
    "\n",
    "My first draft was to use CrossEntropy with soft labels but the model didn't learn\n",
    "\n",
    "Second attempt was Dice loss and the model started training but metrics were low, especially full mask iou, so I switched to Tversky loss to punish FP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss, SoftCrossEntropyLoss, FocalLoss, TverskyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main model code written in lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "class PascalModel(pl.LightningModule):\n",
    "    def __init__(self, encoder_name, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define base torch model inside init\n",
    "        # the classes are hardcoded inside segformer.py \n",
    "        self.model = Segformer(\n",
    "        encoder_name=\"mit_b3\",\n",
    "        encoder_depth=5,\n",
    "        encoder_weights=\"imagenet\",\n",
    "        )\n",
    "\n",
    "        # get preproc from imagenet encoder pretraining\n",
    "        self.params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        \n",
    "        # to tell model not to update this \n",
    "        self.register_buffer(\"std\", torch.tensor(self.params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(self.params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # this is our losses, alpha and beta are same bc changing them between one another\n",
    "        # can lead to overfit so it stays the same for three of them\n",
    "        \n",
    "        self.binary_loss_fn = TverskyLoss(mode=\"multiclass\", smooth=0.05, classes=2, alpha=2, beta=1)\n",
    "        self.triple_loss_fn = TverskyLoss(mode=\"multiclass\", smooth=0.05, classes=3, alpha=2, beta=1)\n",
    "        self.fine_loss_fn = TverskyLoss(mode=\"multiclass\", smooth=0.05, classes=7, alpha=2, beta=1)\n",
    "        \n",
    "        # loss scaling factor. changind them can lead to poorer metrics\n",
    "        # or overfit so let it be even weighting\n",
    "        \n",
    "        self.binary_lambda = 0.33\n",
    "        self.tri_lambda = 0.33\n",
    "        self.full_lambda = 0.33\n",
    "\n",
    "        # calculate miou as torchmetrics class\n",
    "        # we have bg class on output but it shouldn't exist inside our metric calculation\n",
    "        self.miou_binary = MeanIoU(num_classes=2, input_format=\"index\", include_background=False)\n",
    "        self.miou_triple = MeanIoU(num_classes=3, input_format=\"index\", include_background=False)\n",
    "        self.miou_fine = MeanIoU(num_classes=7, input_format=\"index\", include_background=False)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        # forward pass of model normalizes img internally\n",
    "        img = T.Normalize(self.params[\"mean\"], self.params[\"std\"])(img)\n",
    "        return self.model(img)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        training step of model\n",
    "        \n",
    "        batch = (img, bin_mask, tr_mask, full_mask)\n",
    "        \"\"\"\n",
    "        img, bin_mask, tr_mask, full_mask = batch\n",
    "\n",
    "        bin_pred, tr_pred, full_pred = self(img)\n",
    "        \n",
    "        # ensure masks don't have single dim in masks channel\n",
    "        bin_mask = bin_mask.squeeze(1).long()\n",
    "        tr_mask = tr_mask.squeeze(1).long()\n",
    "        full_mask = full_mask.squeeze(1).long()\n",
    "\n",
    "        # weighted losses\n",
    "        loss_bin = self.binary_lambda * self.binary_loss_fn(bin_pred, bin_mask)\n",
    "        loss_tri = self.tri_lambda * self.triple_loss_fn(tr_pred, tr_mask)\n",
    "        loss_full = self.full_lambda * self.fine_loss_fn(full_pred, full_mask)\n",
    "\n",
    "        total_loss = loss_bin + loss_tri + loss_full\n",
    "\n",
    "        self.log(\"train_loss\", total_loss, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"\n",
    "        validation step for logging val loss and IoU metrics\n",
    "        \"\"\"\n",
    "        img, bin_mask, tr_mask, full_mask = batch\n",
    "        \n",
    "        bin_pred, tr_pred, full_pred = self(img)\n",
    "\n",
    "        bin_mask = bin_mask.squeeze(1).long()\n",
    "        tr_mask = tr_mask.squeeze(1).long()\n",
    "        full_mask = full_mask.squeeze(1).long()\n",
    "\n",
    "        loss_bin = self.binary_lambda * self.binary_loss_fn(bin_pred, bin_mask)\n",
    "        loss_tri = self.tri_lambda * self.triple_loss_fn(tr_pred, tr_mask)\n",
    "        loss_full = self.full_lambda * self.fine_loss_fn(full_pred, full_mask)\n",
    "        \n",
    "        val_loss = loss_bin + loss_tri + loss_full\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        # create preds\n",
    "        # shapes: B C H W where C num classes\n",
    "        bin_pred = bin_pred.softmax(dim=1).argmax(dim=1)\n",
    "        tr_pred = tr_pred.softmax(dim=1).argmax(dim=1)\n",
    "        full_pred = full_pred.softmax(dim=1).argmax(dim=1)\n",
    "\n",
    "        # update iou metrics\n",
    "        self.miou_binary.update(bin_pred, bin_mask)\n",
    "        self.miou_triple.update(tr_pred, tr_mask)\n",
    "        self.miou_fine.update(full_pred, full_mask)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        aggregate iou metrics over the entire validation set\n",
    "        \"\"\"\n",
    "        bin_iou = self.miou_binary.compute()\n",
    "        tri_iou = self.miou_triple.compute()\n",
    "        fine_iou = self.miou_fine.compute()\n",
    "\n",
    "        self.log(\"val_bin_iou\", bin_iou, prog_bar=True)\n",
    "        self.log(\"val_trip_iou\", tri_iou, prog_bar=True)\n",
    "        self.log(\"val_full_iou\", fine_iou, prog_bar=True)\n",
    "\n",
    "        # Reset metrics to start fresh next epoch\n",
    "        self.miou_binary.reset()\n",
    "        self.miou_triple.reset()\n",
    "        self.miou_fine.reset()\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        \"\"\"  \n",
    "        Pred step for inference\n",
    "        \"\"\"\n",
    "\n",
    "        img = batch\n",
    "        \n",
    "        # forward pass\n",
    "        bin_pred, tr_pred, full_pred = self(img)\n",
    "\n",
    "        # get predictions\n",
    "        bin_pred_cls = bin_pred.softmax(dim=1).argmax(dim=1)\n",
    "        tr_pred_cls  = tr_pred.softmax(dim=1).argmax(dim=1)\n",
    "        full_pred_cls= full_pred.softmax(dim=1).argmax(dim=1)\n",
    "\n",
    "        return {\n",
    "            \"bin_pred\": bin_pred_cls,\n",
    "            \"tr_pred\": tr_pred_cls,\n",
    "            \"full_pred\": full_pred_cls\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = StableAdamW(self.parameters(), lr=lr, weight_decay=w_reg, \n",
    "                                decouple_lr=True,\n",
    "                                kahan_sum=True)\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=25)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PascalModel(\"mit_b3\", \n",
    "                    encoder_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get useful callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import StochasticWeightAveraging, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early stop if metrics decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping('val_full_iou', mode=\"max\", patience=6)\n",
    "\n",
    "model_chck = ModelCheckpoint(monitor='val_full_iou', save_top_k=2,  mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"/home/linda/mil_test/checkpoints_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we can train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type        | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model          | Segformer   | 44.6 M | train\n",
      "1 | binary_loss_fn | TverskyLoss | 0      | train\n",
      "2 | triple_loss_fn | TverskyLoss | 0      | train\n",
      "3 | fine_loss_fn   | TverskyLoss | 0      | train\n",
      "4 | miou_binary    | MeanIoU     | 0      | train\n",
      "5 | miou_triple    | MeanIoU     | 0      | train\n",
      "6 | miou_fine      | MeanIoU     | 0      | train\n",
      "-------------------------------------------------------\n",
      "44.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.6 M    Total params\n",
      "178.404   Total estimated model params size (MB)\n",
      "583       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  98%|█████████▊| 120/123 [00:46<00:01,  2.58it/s, v_num=22, train_loss=0.396, val_loss=0.435, val_bin_iou=0.626, val_trip_iou=0.414, val_full_iou=0.206]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=epochs,\n",
    "                     # train in bf16\n",
    "                     precision=\"bf16\", accelerator=\"gpu\", check_val_every_n_epoch=3,\n",
    "                     # clip grad norm for stability\n",
    "                     # uncomment if stableadamw\n",
    "                     # gradient_clip_val=0.3,\n",
    "                     callbacks=[model_chck,\n",
    "                                early_stopping],\n",
    "                     # make gradient accumulation for larger BS with increase time\n",
    "                     default_root_dir=\"/home/linda/mil_test/checkpoints_2\",\n",
    "                     logger=logger)\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=test_dl,\n",
    "    # resume training\n",
    "    # ckpt_path=\"/home/linda/mil_test/checkpoints_2/lightning_logs/version_12/checkpoints/epoch=26-step=1593.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now after training we can finally predict our model on same set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\" \".join(name.split(\"_\")).title())\n",
    "\n",
    "        # If it's an image, plot it as RGB\n",
    "        if name == \"image\":\n",
    "            # Convert CHW to HWC for plotting\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            plt.imshow(image)\n",
    "        else:\n",
    "            plt.imshow(image, cmap=\"tab20\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image=image, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can conclude, that using multihead segmentation model works quite good with few resources that I have. We managed to fit segformer into consumer-grade notebook gpu, train in bf16 and get predictions.\n",
    "Final metrics are\n",
    "- binary miou - \n",
    "- triple iou - \n",
    "- full iou - \n",
    "\n",
    "They are not even close to SotA on this dataset\n",
    "There is a way forward to squeeze more from the model, such as training in higher BS, using larger encoder, TTAs, cutmix, mixup, training more, tuning parameters, augmenting images further. Some ideas beyond current setup are:\n",
    "\n",
    "1. Using another loss, more rewarding model on finding exact parts of body and using hierarchial structure of labels to reward leaves of the hierarchial tree. Something like https://github.com/lingorX/HieraSeg/tree/cc3c1cfbabe3cc2af620e0193a245822cca8a841/Pytorch\n",
    "2. Using another model, more complex layer building. Maybe MMSeg package models can make better metrics\n",
    "3. More GPUs, DDP, FSDP, DeepSpeed\n",
    "4. Use SAM model for zero shot more mask creation or even fine tune one instead of traditional methods\n",
    "5. Use 4k image upscale in test time and different image ratios in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
